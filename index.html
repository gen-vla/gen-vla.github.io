<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A framework that enhances the generalization of Vision-Language-Action models by preserving pretrained representations.">
  <meta name="keywords" content="VLA, Vision-Language-Action, Robotics, Generalization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enhancing Generalization in Vision-Language-Action Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <style>
    .publication-title {
      margin-bottom: 1.5rem;
    }
    
    .publication-authors {
      margin-bottom: 0.5rem;
    }
    
    /* Enhanced carousel styles */
    .demo-carousel-container {
      position: relative;
      overflow: hidden;
      border-radius: 12px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
      background: #f8f9fa;
      padding: 2rem;
      margin: 2rem 0;
    }
    
    .demo-carousel-wrapper {
      position: relative;
      overflow: hidden;
    }
    
    .demo-carousel {
      display: flex;
      transition: transform 0.5s ease-in-out;
      gap: 2rem;
    }
    
    .demo-item {
      flex: 0 0 calc(50% - 1rem);
      background: white;
      border-radius: 12px;
      padding: 1.5rem;
      box-shadow: 0 2px 12px rgba(0, 0, 0, 0.08);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
      border: 1px solid #e9ecef;
    }
    
    .demo-item:hover {
      transform: translateY(-4px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
    }
    
    .demo-item img {
      width: 100%;
      height: 200px;
      object-fit: cover;
      border-radius: 8px;
      margin-bottom: 1rem;
      border: 2px solid #f1f3f4;
    }
    
    .demo-item p {
      margin: 0;
      font-weight: 600;
      color: #2c3e50;
      font-size: 1.1rem;
    }
    
    /* Navigation buttons */
    .carousel-nav {
      display: flex;
      justify-content: center;
      gap: 1rem;
      margin-top: 2rem;
    }
    
    .nav-btn {
      background: #3273dc;
      color: white;
      border: none;
      width: 50px;
      height: 50px;
      border-radius: 50%;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 1.2rem;
      transition: all 0.3s ease;
      box-shadow: 0 2px 10px rgba(50, 115, 220, 0.3);
    }
    
    .nav-btn:hover {
      background: #2366d1;
      transform: scale(1.1);
      box-shadow: 0 4px 15px rgba(50, 115, 220, 0.5);
    }
    
    .nav-btn:disabled {
      background: #ccc;
      cursor: not-allowed;
      transform: none;
      box-shadow: none;
    }
    
    /* Pagination dots */
    .carousel-pagination {
      display: flex;
      justify-content: center;
      gap: 0.5rem;
      margin-top: 1rem;
    }
    
    .pagination-dot {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #ccc;
      cursor: pointer;
      transition: all 0.3s ease;
    }
    
    .pagination-dot.active {
      background: #3273dc;
      transform: scale(1.2);
    }
    
    /* Responsive design */
    @media (max-width: 768px) {
      .demo-item {
        flex: 0 0 100%;
      }
      
      .demo-carousel-container {
        padding: 1rem;
      }
      
      .demo-item img {
        height: 150px;
      }
    }
    
    /* Loading animation */
    .demo-item img {
      opacity: 0;
      animation: fadeIn 0.6s ease forwards;
    }
    
    @keyframes fadeIn {
      to {
        opacity: 1;
      }
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://gen-vla.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Shresth Grover¹*,</span>
            <span class="author-block">
              Akshay Gopalkrishnan¹*,</span>
            <span class="author-block">
              Bo Ai¹,
            </span>
            <span class="author-block">
              Henrik I. Christensen¹,
            </span>
            <span class="author-block">
              Hao Su¹,²,
            </span>
            <span class="author-block">
              Xuanlin Li²
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">¹UC San Diego,</span>
            <span class="author-block">²Hillbot</span>
            <span class="author-block">*Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.11417"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://gen-vla.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-video"></i>
                  </span>
                  <span>Project Page</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://gen-vla.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Our framework improves the robustness and generalization of robot manipulation policies by preserving the rich features of pretrained vision-language models.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <img src="https://via.placeholder.com/600x300/4285f4/ffffff?text=VLA+Model+Demo" alt="Demonstration of the VLA model" style="border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.1);">
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation.
          </p>
          <p>
            Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances.
          </p>
          <p>
            Evaluations in simulation and on real robot show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <img src="https://via.placeholder.com/800x400/34a853/ffffff?text=Method+Overview" alt="Method Overview Diagram" style="border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.1);">
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            To address the challenge of representation degradation when fine-tuning VLMs on robotic data, we introduce a framework with three key designs. First, a dual-encoder architecture with a frozen encoder to preserve robust pretrained visual features and a trainable encoder to adapt to specific robot tasks. Second, a string-based action tokenizer that represents continuous robot actions as character sequences, aligning them with the language model's pretraining and allowing for better reuse of its representations. Finally, a co-training strategy that mixes robotic data with vision-language datasets focused on spatial reasoning and affordances, which helps prevent overfitting and improves generalization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluation</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our approach in both simulation and real-world settings to assess its effectiveness in improving generalization and robustness.
          </p>
          <h3 class="title is-4">Simulation Experiments</h3>
          <p>
            In simulation, we utilize the SimplerEnv benchmark to conduct controlled evaluations. We assess performance under two conditions:
          </p>
          <ul>
            <li><strong>Visual Matching (In-Distribution):</strong> This setup uses visuals that closely match the training data, serving as a baseline for performance on familiar environments.</li>
            <li><strong>Visual Variant Aggregation (Out-of-Distribution):</strong> This setup introduces variations in backgrounds, lighting, table textures, distractor objects, and camera poses, testing the model's generalization capabilities to unseen conditions.</li>
          </ul>
          <p>
            We also analyze performance under varying levels of robustness to:
          </p>
          <ul>
            <li><strong>Visual Robustness:</strong> Tested using background masking and visual variant aggregation to evaluate sensitivity to visual changes.</li>
            <li><strong>Language Robustness:</strong> Assessed by using synonymous instructions (generated by GPT-4) to test zero-shot generalization to paraphrased commands.</li>
          </ul>
          <p>
            Results show significant improvements in task success rates and robustness compared to baseline VLA models. Our method demonstrates better performance across diverse tasks, especially when faced with out-of-distribution visuals and varied language instructions.
          </p>

          <h3 class="title is-4">Real-World Evaluation</h3>
          <p>
            To validate our findings in a practical setting, we deploy our models on the ViperX 300s robot, fine-tuned on the Bridge dataset. We evaluate performance with:
          </p>
          <ul>
            <li><strong>In-distribution and Out-of-distribution instructions:</strong> To assess generalization to new task commands.</li>
            <li><strong>Unseen distractors:</strong> Introducing irrelevant objects in the scene to test the robustness of the learned policies.</li>
          </ul>
          <p>
            Our real-world experiments consistently show that our approach outperforms baseline VLAs. The models demonstrate a more robust understanding of tasks, successfully completing goal-conditioned actions even in the presence of semantically or spatially similar distractors, and achieving substantially better performance.
          </p>
        </div>
      </div>
    </div>

    <!-- Enhanced Demonstration Videos Carousel -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Demonstrations</h2>
        <div class="content has-text-centered">
          <p>
            Here we showcase sample demonstrations of our model's performance in various scenarios, highlighting its generalization capabilities.
          </p>
        </div>
        
        <div class="demo-carousel-container">
          <div class="demo-carousel-wrapper">
            <div class="demo-carousel" id="demoCarousel">
              <div class="demo-item">
                <img src="https://via.placeholder.com/400x200/ff6b6b/ffffff?text=Task+1" alt="Demonstration for picking up a carrot and placing it on plate">
                <p class="has-text-centered">Task 1: Put carrot on plate.</p>
              </div>
              <div class="demo-item">
                <img src="https://via.placeholder.com/400x200/4ecdc4/ffffff?text=Task+2" alt="Demonstration for putting knife on cloth">
                <p class="has-text-centered">Task 2: Put knife on cloth.</p>
              </div>
              <div class="demo-item">
                <img src="https://via.placeholder.com/400x200/45b7d1/ffffff?text=Task+3" alt="Demonstration for placing carrot on a plate">
                <p class="has-text-centered">Task 3: Place the carrot on the plate.</p>
              </div>
              <div class="demo-item">
                <img src="https://via.placeholder.com/400x200/f9ca24/ffffff?text=Task+4" alt="Demonstration for placing object on target">
                <p class="has-text-centered">Task 4: Place target on plate.</p>
              </div>
              <div class="demo-item">
                <img src="https://via.placeholder.com/400x200/6c5ce7/ffffff?text=Task+5" alt="Demonstration for complex manipulation">
                <p class="has-text-centered">Task 5: Complex manipulation task.</p>
              </div>
              <div class="demo-item">
                <img src="https://via.placeholder.com/400x200/fd79a8/ffffff?text=Task+6" alt="Demonstration for precision task">
                <p class="has-text-centered">Task 6: Precision manipulation.</p>
              </div>
            </div>
          </div>
          
          <div class="carousel-nav">
            <button class="nav-btn" id="prevBtn">
              <i class="fas fa-chevron-left"></i>
            </button>
            <button class="nav-btn" id="nextBtn">
              <i class="fas fa-chevron-right"></i>
            </button>
          </div>
          
          <div class="carousel-pagination" id="pagination"></div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{grover2025enhancing,
  title={Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations},
  author={Grover, Shresth and Gopalkrishnan, Akshay and Ai, Bo and Christensen, Henrik I. and Su, Hao and Li, Xuanlin},
  journal={arXiv preprint arXiv:2509.11417},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
document.addEventListener('DOMContentLoaded', function() {
  const carousel = document.getElementById('demoCarousel');
  const prevBtn = document.getElementById('prevBtn');
  const nextBtn = document.getElementById('nextBtn');
  const pagination = document.getElementById('pagination');
  const items = carousel.querySelectorAll('.demo-item');
  
  let currentIndex = 0;
  const itemsPerView = window.innerWidth <= 768 ? 1 : 2;
  const maxIndex = Math.max(0, items.length - itemsPerView);
  
  // Create pagination dots
  function createPagination() {
    pagination.innerHTML = '';
    const totalPages = Math.ceil(items.length / itemsPerView);
    
    for (let i = 0; i < totalPages; i++) {
      const dot = document.createElement('div');
      dot.classList.add('pagination-dot');
      if (i === 0) dot.classList.add('active');
      dot.addEventListener('click', () => goToSlide(i * itemsPerView));
      pagination.appendChild(dot);
    }
  }
  
  // Update carousel position
  function updateCarousel() {
    const translateX = -currentIndex * (100 / itemsPerView);
    carousel.style.transform = `translateX(${translateX}%)`;
    
    // Update pagination
    const dots = pagination.querySelectorAll('.pagination-dot');
    const currentPage = Math.floor(currentIndex / itemsPerView);
    dots.forEach((dot, index) => {
      dot.classList.toggle('active', index === currentPage);
    });
    
    // Update button states
    prevBtn.disabled = currentIndex === 0;
    nextBtn.disabled = currentIndex >= maxIndex;
  }
  
  // Go to specific slide
  function goToSlide(index) {
    currentIndex = Math.max(0, Math.min(index, maxIndex));
    updateCarousel();
  }
  
  // Event listeners
  prevBtn.addEventListener('click', () => {
    if (currentIndex > 0) {
      currentIndex -= itemsPerView;
      if (currentIndex < 0) currentIndex = 0;
      updateCarousel();
    }
  });
  
  nextBtn.addEventListener('click', () => {
    if (currentIndex < maxIndex) {
      currentIndex += itemsPerView;
      if (currentIndex > maxIndex) currentIndex = maxIndex;
      updateCarousel();
    }
  });
  
  // Touch/swipe support
  let startX = 0;
  let isDragging = false;
  
  carousel.addEventListener('touchstart', (e) => {
    startX = e.touches[0].clientX;
    isDragging = true;
  });
  
  carousel.addEventListener('touchmove', (e) => {
    if (!isDragging) return;
    e.preventDefault();
  });
  
  carousel.addEventListener('touchend', (e) => {
    if (!isDragging) return;
    isDragging = false;
    
    const endX = e.changedTouches[0].clientX;
    const diff = startX - endX;
    
    if (Math.abs(diff) > 50) {
      if (diff > 0 && currentIndex < maxIndex) {
        currentIndex += itemsPerView;
        if (currentIndex > maxIndex) currentIndex = maxIndex;
      } else if (diff < 0 && currentIndex > 0) {
        currentIndex -= itemsPerView;
        if (currentIndex < 0) currentIndex = 0;
      }
      updateCarousel();
    }
  });
  
  // Keyboard navigation
  document.addEventListener('keydown', (e) => {
    if (e.key === 'ArrowLeft') {
      prevBtn.click();
    } else if (e.key === 'ArrowRight') {
      nextBtn.click();
    }
  });
  
  // Responsive handling
  window.addEventListener('resize', () => {
    const newItemsPerView = window.innerWidth <= 768 ? 1 : 2;
    if (newItemsPerView !== itemsPerView) {
      location.reload(); // Simple approach for responsive changes
    }
  });
  
  // Initialize
  createPagination();
  updateCarousel();
});
</script>

</body>
</html>
