<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A framework that enhances the generalization of Vision-Language-Action models by preserving pretrained representations.">
  <meta name="keywords" content="VLA, Vision-Language-Action, Robotics, Generalization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enhancing Generalization in Vision-Language-Action Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://gen-vla.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Shresth Grover¹*,</span>
            <span class="author-block">
              Akshay Gopalkrishnan¹*,</span>
            <span class="author-block">
              Bo Ai¹,
            </span>
            <span class="author-block">
              Henrik I. Christensen¹,
            </span>
            <span class="author-block">
              Hao Su¹,²,
            </span>
            <span class="author-block">
              Xuanlin Li²
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">¹UC San Diego,</span>
            <span class="author-block">²Hillbot</span>
            <span class="author-block">*Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.11417"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://gen-vla.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-video"></i>
                  </span>
                  <span>Project Page</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://gen-vla.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Our framework improves the robustness and generalization of robot manipulation policies by preserving the rich features of pretrained vision-language models.
      </h2>
    </div>
  </div>
</section>

<section class="section">                                                          <div class="container is-max-desktop">                                             <div class="columns is-centered">                                                  <div class="column has-text-centered">                                             <img src="./videos/promo_.gif" alt="Demonstration of the VLA model">           </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation.
          </p>
          <p>
            Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances.
          </p>
          <p>
            Evaluations in simulation and on real robot show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
         <!-- Note: Most browsers cannot render a PDF file directly in an <img> tag. -->
         <!-- You may need to convert method.pdf to a PNG or JPG for it to display correctly. -->                                                                         <img src="./images/figure2-method.png" alt="Method Overview Diagram">
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <p>
            To address the challenge of representation degradation when fine-tuning VLMs on robotic data, we introduce a framework with three key designs. First, a dual-encoder architecture with a frozen encoder to preserve robust pretrained visual features and a trainable encoder to adapt to specific robot tasks. Second, a string-based action tokenizer that represents continuous robot actions as character sequences, aligning them with the language model's pretraining and allowing for better reuse of its representations. Finally, a co-training strategy that mixes robotic data with vision-language datasets focused on spatial reasoning and affordances, which helps prevent overfitting and improves generalization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image Section -->
<!--/ Image Section -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Evaluation</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate our approach in both simulation and real-world settings to assess its effectiveness in improving generalization and robustness.
          </p>
          <h3 class="title is-4">Simulation Experiments</h3>
          <p>
            In simulation, we utilize the SimplerEnv benchmark to conduct controlled evaluations. We assess performance under two conditions:
          </p>
          <ul>
            <li><strong>Visual Matching (In-Distribution):</strong> This setup uses visuals that closely match the training data, serving as a baseline for performance on familiar environments.</li>
            <li><strong>Visual Variant Aggregation (Out-of-Distribution):</strong> This setup introduces variations in backgrounds, lighting, table textures, distractor objects, and camera poses, testing the model's generalization capabilities to unseen conditions.</li>
          </ul>
          <p>
            We also analyze performance under varying levels of robustness to:
          </p>
          <ul>
            <li><strong>Visual Robustness:</strong> Tested using background masking and visual variant aggregation to evaluate sensitivity to visual changes.</li>
            <li><strong>Language Robustness:</strong> Assessed by using synonymous instructions (generated by GPT-4) to test zero-shot generalization to paraphrased commands.</li>
          </ul>
          <p>
            Results show significant improvements in task success rates and robustness compared to baseline VLA models. Our method demonstrates better performance across diverse tasks, especially when faced with out-of-distribution visuals and varied language instructions.
          </p>

          <h3 class="title is-4">Real-World Evaluation</h3>
          <p>
            To validate our findings in a practical setting, we deploy our models on the ViperX 300s robot, fine-tuned on the Bridge dataset. We evaluate performance with:
          </p>
          <ul>
            <li><strong>In-distribution and Out-of-distribution instructions:</strong> To assess generalization to new task commands.</li>
            <li><strong>Unseen distractors:</strong> Introducing irrelevant objects in the scene to test the robustness of the learned policies.</li>
          </ul>
          <p>
            Our real-world experiments consistently show that our approach outperforms baseline VLAs. The models demonstrate a more robust understanding of tasks, successfully completing goal-conditioned actions even in the presence of semantically or spatially similar distractors, and achieving substantially better performance.
          </p>
        </div>
      </div>
    </div>

    <!-- Demonstration GIFs (Scrollable) -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Demonstrations</h2>
        <div class="content has-text-centered">
          <p>
            Here we showcase sample demonstrations of our model's performance in various scenarios, highlighting its generalization capabilities.
          </p>
        </div>
        <div id="demo-carousel" class="carousel results-carousel">
          <div class="item item-demo-1">
            <img src="./videos/video_1.gif" alt="Demonstration for picking up a carrot and placing it on plate">
            <p class="has-text-centered">Task 1: Put carrot on plate.</p>
          </div>
          <div class="item item-demo-2">
            <img src="./videos/video_2.gif" alt="Demonstration for putting knife on cloth">
            <p class="has-text-centered">Task 2: Put knife on cloth.</p>
          </div>
          <div class="item item-demo-3">
            <img src="./videos/video_3.gif" alt="Demonstration for placing carrot on a plate">
            <p class="has-text-centered">Task 3: Place the carrot on the plate.</p>
          </div>
          <div class="item item-demo-4">
            <img src="./videos/video_6.gif" alt="Demonstration for placing objct on target">
            <p class="has-text-centered">Task 4: Place target on plate.</p>
          </div>
          <!-- Add more GIF items as needed -->
        </div>
      </div>
    </div>
    <!--/ Demonstration GIFs (Scrollable) -->

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{grover2025enhancing,
  title={Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations},
  author={Grover, Shresth and Gopalkrishnan, Akshay and Ai, Bo and Christensen, Henrik I. and Su, Hao and Li, Xuanlin},
  journal={arXiv preprint arXiv:2509.11417},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
<script>
  // Initialize the carousel for demonstrations
  document.addEventListener('DOMContentLoaded', function() {
    bulmaCarousel.attach('#demo-carousel', {
      slidesToShow: 2,      // Show 2 GIFs at a time
      slidesToScroll: 2,    // Scroll 2 GIFs at a time
      loop: true,           // Allow the carousel to loop
      autoplay: false,      // Do not automatically play
      navigation: true,     // Show next/prev buttons
      pagination: true,     // Show pagination dots
    });
  });
</script>
</body>
</html>
