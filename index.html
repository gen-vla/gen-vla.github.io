<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A framework that enhances the generalization of Vision-Language-Action models by preserving pretrained representations.">
  <meta name="keywords" content="VLA, Vision-Language-Action, Robotics, Generalization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Enhancing Generalization in Vision-Language-Action Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    /* Video container with progress bar */
    .video-container {
      position: relative;
      display: inline-block;
      width: 640px;
      max-width: 100%;
    }

    .video-container video {
      display: block;
      width: 100%;
      height: auto;
    }

    .video-progress-bar {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: 4px;
      background-color: rgba(255, 255, 255, 0.3);
      cursor: pointer;
    }

    .video-progress-fill {
      height: 100%;
      background-color: #3273dc;
      width: 0%;
      transition: width 0.1s linear;
    }

    /* Enhanced carousel styles */
    .demo-carousel {
      position: relative;
      overflow: hidden;
      margin: 2rem 0;
    }

    .carousel-container {
      display: flex;
      transition: transform 0.5s ease;
      gap: 1rem;
    }

    .carousel-item {
      flex: 0 0 48%;
      padding: 1rem;
      background: #f8f9fa;
      border-radius: 12px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
      transition: transform 0.3s ease, box-shadow 0.3s ease;
    }

    .carousel-item:hover {
      transform: translateY(-5px);
      box-shadow: 0 8px 15px rgba(0, 0, 0, 0.15);
    }

    .carousel-item img {
      width: 100%;
      height: 200px;
      object-fit: cover;
      border-radius: 8px;
      margin-bottom: 1rem;
    }

    .carousel-item p {
      font-weight: 600;
      color: #363636;
      margin: 0;
      text-align: center;
    }

    .carousel-nav {
      display: flex;
      justify-content: center;
      align-items: center;
      margin-top: 1.5rem;
      gap: 1rem;
    }

    .carousel-btn {
      background: #3273dc;
      color: white;
      border: none;
      width: 40px;
      height: 40px;
      border-radius: 50%;
      cursor: pointer;
      font-size: 1.2rem;
      transition: background-color 0.3s ease, transform 0.2s ease;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    .carousel-btn:hover {
      background: #2366d1;
      transform: scale(1.1);
    }

    .carousel-btn:disabled {
      background: #dbdbdb;
      cursor: not-allowed;
      transform: none;
    }

    .carousel-indicators {
      display: flex;
      gap: 0.5rem;
    }

    .carousel-dot {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      background: #dbdbdb;
      cursor: pointer;
      transition: background-color 0.3s ease, transform 0.2s ease;
    }

    .carousel-dot.active {
      background: #3273dc;
      transform: scale(1.2);
    }

    .carousel-dot:hover {
      background: #2366d1;
    }

    @media (max-width: 768px) {
      .carousel-item {
        flex: 0 0 100%;
      }

      .carousel-item img {
        height: 150px;
      }
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                    <a href="https://shroglck.github.io/" class="author-link">Shresth Grover</a>¹*,
            </span>
            <span class="author-block">
                    <a href="https://akshaygopalkr.github.io/" class="author-link">Akshay Gopalkrishnan</a>¹*,</span>
            <span class="author-block">
              <a href="https://albertboai.com" class="author-link">Bo Ai</a>¹,
            </span>
            <span class="author-block">
              <a href="http://hichristensen.net/" class="author-link">Henrik I. Christensen</a>¹,
            </span>
            <span class="author-block">
                        <a href="https://cseweb.ucsd.edu/~haosu/index.html" class="author-link">Hao Su</a>¹²,
            </span>
            <span class="author-block">
                     <a href="https://xuanlinli17.github.io/" class="author-link">Xuanlin Li</a>²
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">¹UC San Diego,</span>
            <span class="author-block">²Hillbot</span>
            <span class="author-block">*Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2509.11417"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://gen-vla.github.io"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-video"></i>
                  </span>
                  <span>Project Page</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/akshaygopalkr/vla-plus"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(Coming Soon)</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Our framework improves the robustness and generalization of robot manipulation policies by preserving the rich features of pretrained vision-language models.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <div class="video-container">
          <video id="promo-video" autoplay loop muted playsinline width="640" height="360">
            <source src="./videos/vla_promo.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
          <div class="video-progress-bar" id="progress-bar">
            <div class="video-progress-fill" id="progress-fill"></div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-language-action (VLA) models, finetuned from powerful pretrained vision-language models (VLMs), promise to create generalist robots. However, this finetuning process often degrades the very representations that make them powerful, limiting generalization. We propose a framework that preserves these pretrained features while adapting them for robot manipulation. Our method introduces a dual-encoder design to retain features, a string-based action tokenizer to align actions with language, and a co-training strategy to balance robot and vision-language data. Our evaluations show significant improvements in robustness, generalization, and overall task success.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
         <img src="./images/method_2.png" alt="Method Overview Diagram">
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Method</h2>
        <div class="content has-text-justified">
          <p>
            Our framework is built on three key ideas to prevent representation degradation. <strong>(1) Partially-Frozen Visual Encoders:</strong> We use two encoders—one frozen to preserve robust, pretrained VLM features and one trainable to adapt to the specific robot task. <strong>(2) String-Based Action Tokenizer:</strong> We represent continuous robot actions as strings, unifying them with the text-based pretraining of the language model. <strong>(3) Co-Training Strategy:</strong> We mix robot demonstration data with vision-language datasets that emphasize spatial reasoning, preventing the model from overfitting to robot-specific data and enhancing its generalization capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Key Findings & Results</h2>

    <!-- Generalization Three Plots -->
    <div class="columns is-centered is-vcentered">
        <div class="column is-one-third has-text-centered">
            <img src="./images/vis_matching.png" alt="Bar chart for Visual Matching">
            <p class="is-size-7"><em>SimplerEnv Visual Matching</em></p>
        </div>
        <div class="column is-one-third has-text-centered">
            <img src="./images/var_agg_perf.png" alt="Bar chart for Visual Variant Aggregation">
            <p class="is-size-7"><em>SimplerEnv Variant Aggregation (OOD)</em></p>
        </div>
        <div class="column is-one-third has-text-centered">
            <img src="./images/text_ag.png" alt="Bar chart for Language Robustness">
            <p class="is-size-7"><em>Language Robustness</em></p>
        </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4 has-text-centered">Improving Generalization and Robustness</h3>
        <p class="has-text-justified">
          A key challenge for robots is generalizing to novel scenes and instructions. Baseline models show a significant drop in performance when backgrounds change or instructions are paraphrased. Our approach demonstrates substantially stronger generalization, maintaining high success rates even with out-of-distribution (OOD) visual and language inputs.
        </p>
      </div>
    </div>

    <!-- Real World and VQA Performance -->
    <div class="columns is-centered is-vcentered">
      <div class="column is-6">
        <img src="./images/real_perf.png" alt="Qualitative results of robot picking knife and carrot">
        <p class="is-size-7 has-text-centered"><em>Real World Performance.</em></p>
      </div>
      <div class="column is-6">
        <img src="./images/vlm_benchmark_results.png" alt="Radar chart of VQA benchmark performance">
        <p class="is-size-7 has-text-centered"><em>VQA Benchmark Performance.</em></p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4 has-text-centered">Robust Real-World Performance and Retained Reasoning Capabilities</h3>
        <p class="has-text-justified">
          In real-world tests, our models consistently outperform baselines, especially in the presence of distracting objects. While baseline models often get confused by distractors (e.g., picking a carrot instead of a knife), our model demonstrates a more robust understanding of the task, successfully completing the instructed action. Additionally, standard VLA finetuning harms the model's ability to perform general visual reasoning. Our training recipe allows the model to retain significantly higher performance on standard VQA benchmarks (solid lines vs. dashed lines), demonstrating that it doesn't just learn robotic actions but also preserves its core reasoning abilities.
        </p>
      </div>
    </div>

    <!-- t-SNE Visualization -->
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <img src="./images/tsne_new.png" alt="t-SNE visualizations comparing feature representations" style="max-width: 85%;">
        <p class="is-size-7"><em>t-SNE visualizations of vision encoder features on CIFAR-10.</em></p>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h3 class="title is-4 has-text-centered">Preserving Semantic Structure in Visual Representations</h3>
        <p class="has-text-justified">
          We visualize how different training approaches affect the learned visual representations using t-SNE on CIFAR-10. Comparing (i) the original visual backbone from the VLM before VLA training, (ii) after direct VLA fine-tuning on robot data, and (iii) after applying our approaches, we observe that our method yields noticeably tighter and more well-separated class clusters. The numbers in the visualization indicate linear-probe classification performance on CIFAR-10 using the corresponding features. For both OpenVLA and π0 models, our approach leads to better linear-probe performance, indicating superior preservation of semantic structures in pretrained visual representations. This demonstrates that our framework maintains the rich representational capacity of the original vision-language models while adapting them for robotic tasks.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Enhanced Demonstration Carousel -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Demonstrations</h2>
        <div class="content has-text-centered">
          <p>
            Here we showcase sample demonstrations of our model's performance in various scenarios, highlighting its generalization capabilities.
          </p>
        </div>

        <div class="demo-carousel">
          <div class="carousel-container" id="carousel-container">
            <div class="carousel-item">
              <img src="./videos/video_1.gif" alt="Demonstration for picking up a carrot and placing it on plate">
              <p>Task 1: Put carrot on plate.</p>
            </div>
            <div class="carousel-item">
              <img src="./videos/video_2.gif" alt="Demonstration for putting knife on cloth">
              <p>Task 2: Put knife on cloth.</p>
            </div>
            <div class="carousel-item">
              <img src="./videos/video_3.gif" alt="Demonstration for placing carrot on a plate">
              <p>Task 3: Place the carrot on the plate.</p>
            </div>
            <div class="carousel-item">
              <img src="./videos/video_6.gif" alt="Demonstration for placing object on target">
              <p>Task 4: Place carrot on plate.</p>
            </div>
            <div class="carousel-item">
              <img src="./videos/video_7.gif" alt="Additional demonstration">
              <p>Task 5: Put carrot on plate.</p>
            </div>
            <div class="carousel-item">
              <img src="./videos/video_5.gif" alt="Additional demonstration">
              <p>Task 6: Put carrot on yellow plate.</p>
            </div>
          </div>

          <div class="carousel-nav">
            <button class="carousel-btn" id="prev-btn">‹</button>
            <div class="carousel-indicators" id="carousel-indicators"></div>
            <button class="carousel-btn" id="next-btn">›</button>
          </div>
        </div>
      </div>
    </div>
    <!--/ Enhanced Demonstration Carousel -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{grover2025enhancing,
  title={Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations},
  author={Grover, Shresth and Gopalkrishnan, Akshay and Ai, Bo and Christensen, Henrik I. and Su, Hao and Li, Xuanlin},
  journal={arXiv preprint arXiv:2509.11417},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
class DemoCarousel {
  constructor() {
    this.container = document.getElementById('carousel-container');
    if (!this.container) return; // Exit if carousel doesn't exist
    this.prevBtn = document.getElementById('prev-btn');
    this.nextBtn = document.getElementById('next-btn');
    this.indicators = document.getElementById('carousel-indicators');
    this.items = this.container.querySelectorAll('.carousel-item');
    this.currentIndex = 0;

    this.init();
  }

  init() {
    this.updateItemsPerPage();
    this.createIndicators();
    this.bindEvents();
    this.updateCarousel();
    this.updateButtons();
  }

  updateItemsPerPage() {
      this.itemsPerPage = window.innerWidth <= 768 ? 1 : 2;
      this.totalPages = Math.ceil(this.items.length / this.itemsPerPage);
  }

  createIndicators() {
    if (!this.indicators) return;
    this.indicators.innerHTML = '';
    for (let i = 0; i < this.totalPages; i++) {
      const dot = document.createElement('div');
      dot.className = 'carousel-dot';
      dot.addEventListener('click', () => this.goToPage(i));
      this.indicators.appendChild(dot);
    }
  }

  bindEvents() {
    if (!this.prevBtn || !this.nextBtn) return;
    this.prevBtn.addEventListener('click', () => this.prevPage());
    this.nextBtn.addEventListener('click', () => this.nextPage());

    // Handle window resize
    window.addEventListener('resize', () => {
      this.updateItemsPerPage();
      this.currentIndex = Math.min(this.currentIndex, this.totalPages - 1);
      this.createIndicators();
      this.updateCarousel();
      this.updateButtons();
    });

    // Touch/swipe support
    let startX = 0;
    let startY = 0;
    let currentX = 0;
    let currentY = 0;

    this.container.addEventListener('touchstart', (e) => {
      startX = e.touches[0].clientX;
      startY = e.touches[0].clientY;
    }, { passive: true });

    this.container.addEventListener('touchmove', (e) => {
      currentX = e.touches[0].clientX;
      currentY = e.touches[0].clientY;
    }, { passive: true });

    this.container.addEventListener('touchend', () => {
      const diffX = startX - currentX;
      const diffY = startY - currentY;

      // Only register horizontal swipes
      if (Math.abs(diffX) > Math.abs(diffY) && Math.abs(diffX) > 50) {
        if (diffX > 0) {
          this.nextPage();
        } else {
          this.prevPage();
        }
      }
    }, { passive: true });
  }

  updateCarousel() {
    const itemWidthPercentage = 100 / this.items.length;
    const containerWidthPercentage = itemWidthPercentage * this.itemsPerPage;
    const offset = this.currentIndex * (100 / this.totalPages);

    this.container.style.transform = `translateX(-${offset}%)`;

    const dots = this.indicators.querySelectorAll('.carousel-dot');
    dots.forEach((dot, index) => {
      dot.classList.toggle('active', index === this.currentIndex);
    });
  }

  updateButtons() {
    if (!this.prevBtn || !this.nextBtn) return;
    this.prevBtn.disabled = this.currentIndex === 0;
    this.nextBtn.disabled = this.currentIndex === this.totalPages - 1;
  }

  prevPage() {
    if (this.currentIndex > 0) {
      this.currentIndex--;
      this.updateCarousel();
      this.updateButtons();
    }
  }

  nextPage() {
    if (this.currentIndex < this.totalPages - 1) {
      this.currentIndex++;
      this.updateCarousel();
      this.updateButtons();
    }
  }

  goToPage(index) {
    if (index >= 0 && index < this.totalPages) {
      this.currentIndex = index;
      this.updateCarousel();
      this.updateButtons();
    }
  }
}

// Initialize scripts when DOM is loaded
document.addEventListener('DOMContentLoaded', () => {
  new DemoCarousel();
  
  // Video progress bar functionality
  const video = document.getElementById('promo-video');
  const progressBar = document.getElementById('progress-bar');
  const progressFill = document.getElementById('progress-fill');

  if (video && progressBar && progressFill) {
    // Update progress bar as video plays
    video.addEventListener('timeupdate', () => {
      if (!isNaN(video.duration)) {
        const progress = (video.currentTime / video.duration) * 100;
        progressFill.style.width = progress + '%';
      }
    });

    // Seek video when clicking on progress bar
    progressBar.addEventListener('click', (e) => {
      if (!isNaN(video.duration)) {
        const rect = progressBar.getBoundingClientRect();
        const clickX = e.clientX - rect.left;
        const percentage = clickX / rect.width;
        video.currentTime = percentage * video.duration;
      }
    });
  }
});
</script>
</body>
</html>